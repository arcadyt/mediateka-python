services:
  torchserve:
    image: pytorch/torchserve:0.12.0-gpu
    networks:
      - kafka_network
      - inference_network
    volumes:
      - ./torchserve-service/app/shared:/app:ro,delegated
    ports:
      - "8080:8080"  # TorchServe Inference API
      - "8081:8081"  # TorchServe Management API
      - "8082:8082"  # TorchServe Metrics API
      - "5678:5678"  # Debugging Port
    environment:
      - TORCHSERVE_HOME=/app
      - NVIDIA_VISIBLE_DEVICES=all  # Use GPU
      - CUDA_VISIBLE_DEVICES=0
      - TS_NUMBER_OF_GPU=1
    working_dir: "/app"
    entrypoint: [ "/bin/bash", "/app/startup.sh" ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

networks:
  inference_network:
    name: inference_network
    driver: bridge
  kafka_network:
    name: kafka_network
    driver: bridge
